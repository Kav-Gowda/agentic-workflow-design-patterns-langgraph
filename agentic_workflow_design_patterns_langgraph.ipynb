{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG64XEl6NKb7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Core libraries for agentic workflows and LangGraph\n",
        "!pip install \"langgraph==0.6.6\" \"langchain==0.3.7\" \"langchain-community==0.3.7\"\n",
        "\n",
        "# Embeddings / models / vector search (all free/open-source friendly)\n",
        "!pip install \"sentence-transformers==3.1.1\" \"faiss-cpu==1.8.0.post1\"\n",
        "\n",
        "# For visualizing workflows/graphs\n",
        "!apt-get update\n",
        "!apt-get install -y graphviz libgraphviz-dev\n",
        "!pip install \"pygraphviz==1.14\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the core pieces of LangGraph that I'll need to build the workflow\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# I'll use TypedDict to define the structured state that flows through my graph\n",
        "from typing import TypedDict\n",
        "\n",
        "# Using Pydantic models to keep inputs/outputs clean and validated\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Keeping things provider-agnostic for now; I'll plug in the actual LLM later\n",
        "# (either a small Hugging Face model or a free local backend in Colab)\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n"
      ],
      "metadata": {
        "id": "VFKCw98UOUDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I’m adding a small helper function so I can quickly inspect any workflow I build.\n",
        "# This makes it easier for me to debug the graph structure as it gets more complex.\n",
        "\n",
        "def print_workflow_info(workflow, app=None):\n",
        "    \"\"\"Utility to print out the key details of a LangGraph workflow so I can understand the structure at a glance.\"\"\"\n",
        "\n",
        "    print(\"WORKFLOW INFORMATION\")\n",
        "    print(\"====================\")\n",
        "\n",
        "    # Basic overview of the graph\n",
        "    print(f\"Nodes: {workflow.nodes}\")\n",
        "    print(f\"Edges: {workflow.edges}\")\n",
        "\n",
        "    # Checking how the workflow defines its finish points\n",
        "    try:\n",
        "        finish_points = workflow.finish_points\n",
        "        print(f\"Finish points: {finish_points}\")\n",
        "    except:\n",
        "        try:\n",
        "            # Some versions expose the attribute differently, so I’m covering that too\n",
        "            print(f\"Finish point: {workflow._finish_point}\")\n",
        "        except:\n",
        "            print(\"Finish points attribute not directly accessible\")\n",
        "\n",
        "    # If I'm passing the compiled app, I can also visualize the graph\n",
        "    if app:\n",
        "        print(\"\\nWorkflow Visualization:\")\n",
        "        from IPython.display import display\n",
        "        display(app.get_graph().draw_png())\n"
      ],
      "metadata": {
        "id": "ixScIwHLOUAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For this project I don’t want to rely on paid APIs, so I’m loading a small open-source chat model instead.\n",
        "# This keeps the notebook fully runnable in Colab.\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# I’m using a tiny model so it runs comfortably on Colab’s free GPU/CPU.\n",
        "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Loading tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",              # lets Colab decide CPU/GPU placement\n",
        "    torch_dtype=\"auto\"              # keeps memory usage manageable\n",
        ")\n",
        "\n",
        "# Wrapping the model into a LangChain-compatible pipeline\n",
        "chat_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "# This is the LLM object I'll use throughout the workflow\n",
        "llm = HuggingFacePipeline(pipeline=chat_pipeline)\n"
      ],
      "metadata": {
        "id": "W3HW-LCVOT82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I’m defining the state that will move through my LangGraph workflow.\n",
        "# Keeping it typed helps me stay organized as the graph grows.\n",
        "\n",
        "class ChainState(TypedDict):\n",
        "    job_description: str\n",
        "    resume_summary: str\n",
        "    cover_letter: str\n"
      ],
      "metadata": {
        "id": "2TRXrcjIOT6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function generates a resume summary based on the job description.\n",
        "# I’m keeping the prompt simple and focused so it works well with open-source models too.\n",
        "\n",
        "def generate_resume_summary(state: ChainState) -> ChainState:\n",
        "    prompt = f\"\"\"\n",
        "You are a resume assistant. Read the job description below and produce a short,\n",
        "strong resume-style summary that reflects what an ideal applicant would highlight.\n",
        "\n",
        "Job Description:\n",
        "{state['job_description']}\n",
        "\"\"\"\n",
        "\n",
        "    # For HuggingFacePipeline models, calling the LLM directly returns a string,\n",
        "    # so I don’t need to access `.content` like with OpenAI models.\n",
        "    response = llm(prompt)\n",
        "\n",
        "    return {**state, \"resume_summary\": response}\n"
      ],
      "metadata": {
        "id": "hivll1t9OT31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This node generates a cover letter based on the resume summary and job description.\n",
        "# I’m keeping the prompt clear so the model has enough structure to produce a good output.\n",
        "\n",
        "def generate_cover_letter(state: ChainState) -> ChainState:\n",
        "    prompt = f\"\"\"\n",
        "You are a cover-letter assistant. Using the resume summary below, write a\n",
        "professional, confident, and personalized cover letter for the job.\n",
        "\n",
        "Resume Summary:\n",
        "{state['resume_summary']}\n",
        "\n",
        "Job Description:\n",
        "{state['job_description']}\n",
        "\"\"\"\n",
        "\n",
        "    # HuggingFacePipeline returns plain text, so I just call it directly.\n",
        "    response = llm(prompt)\n",
        "\n",
        "    return {**state, \"cover_letter\": response}\n"
      ],
      "metadata": {
        "id": "1lE4VOSeOT1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I’m setting up the LangGraph workflow and telling it\n",
        "# what kind of state it will pass around between nodes.\n",
        "\n",
        "workflow = StateGraph(ChainState)\n",
        "\n",
        "# Just returning it here so I can quickly inspect the object in the notebook.\n",
        "workflow\n"
      ],
      "metadata": {
        "id": "BaLysidjOTy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding my two main nodes to the workflow. Each node handles one step\n",
        "# in the overall agentic process I'm building.\n",
        "\n",
        "workflow.add_node(\"generate_resume_summary\", generate_resume_summary)\n",
        "workflow.add_node(\"generate_cover_letter\", generate_cover_letter)\n"
      ],
      "metadata": {
        "id": "YAX7h0DjOTwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the first step of my workflow. The graph will always start\n",
        "# by generating the resume summary before moving on to anything else.\n",
        "\n",
        "workflow.set_entry_point(\"generate_resume_summary\")\n"
      ],
      "metadata": {
        "id": "Vp8ab-WbOTtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once the resume summary is generated, I want the workflow to\n",
        "# automatically move to the cover-letter step, so I'm connecting the nodes.\n",
        "\n",
        "workflow.add_edge(\"generate_resume_summary\", \"generate_cover_letter\")\n"
      ],
      "metadata": {
        "id": "Z2HxZtkNOTrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I'm telling the graph where it should stop. Once the cover letter is generated,\n",
        "# the workflow has everything it needs, so that becomes my finish point.\n",
        "\n",
        "workflow.set_finish_point(\"generate_cover_letter\")\n",
        "\n",
        "# Quick check to make sure the graph looks the way I expect.\n",
        "print_workflow_info(workflow)\n"
      ],
      "metadata": {
        "id": "riillAqHOTgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the workflow so it becomes a runnable LangGraph app.\n",
        "# This turns the structure I defined above into an executable pipeline.\n",
        "\n",
        "app = workflow.compile()\n"
      ],
      "metadata": {
        "id": "79XDxsbQPbwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Just visualizing the workflow here so I can see the nodes and edges clearly.\n",
        "# This helps me double-check that the structure matches what I intended.\n",
        "\n",
        "display(Image(app.get_graph().draw_png()))\n"
      ],
      "metadata": {
        "id": "saYXI9uHPbsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I'm preparing a simple test input so I can run the full workflow.\n",
        "# This helps me confirm that both nodes (resume + cover letter) are working correctly.\n",
        "\n",
        "input_state = {\n",
        "    \"job_description\": (\n",
        "        \"We are looking for a data scientist with experience in machine learning, NLP, and Python. \"\n",
        "        \"Prior work with large datasets and experience deploying models into production is required.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "# Running the graph with my input to see if everything connects the right way.\n",
        "result = app.invoke(input_state)\n",
        "\n",
        "# Checking the generated resume summary\n",
        "result['resume_summary']\n"
      ],
      "metadata": {
        "id": "7C4P29zhPbqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a new state type for the routing workflow.\n",
        "# This one will let me classify what the user wants and route it to the right step.\n",
        "\n",
        "class RouterState(TypedDict):\n",
        "    user_input: str\n",
        "    task_type: str\n",
        "    output: str\n"
      ],
      "metadata": {
        "id": "eCQMr2M3Pbmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I’m defining a simple Router schema so I know exactly what the model\n",
        "# is supposed to output whenever I ask it to classify a task.\n",
        "\n",
        "class Router(BaseModel):\n",
        "    role: str = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"Decide whether the user wants to summarize a passage (output 'summarize') \"\n",
        "            \"or translate text into French (output 'translate').\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Since HuggingFace models don’t have built-in tool-calling like OpenAI,\n",
        "# I’m writing a small helper that prompts the model and forces a clean decision.\n",
        "\n",
        "def route_task(user_input: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "You are a routing assistant. Read the user input and decide the task type.\n",
        "\n",
        "If the user wants a summary → respond ONLY with: summarize\n",
        "If the user wants French translation → respond ONLY with: translate\n",
        "\n",
        "User Input:\n",
        "{user_input}\n",
        "\n",
        "Respond with one word only.\n",
        "\"\"\"\n",
        "    # Getting the raw text output from my HF model\n",
        "    raw_output = llm(prompt).strip().lower()\n",
        "\n",
        "    # Cleaning the output so it fits my Router model\n",
        "    if \"summar\" in raw_output:\n",
        "        return \"summarize\"\n",
        "    if \"translat\" in raw_output or \"french\" in raw_output:\n",
        "        return \"translate\"\n",
        "\n",
        "    # Fallback in case the model is indecisive\n",
        "    return \"summarize\"\n",
        "\n",
        "# Quick test\n",
        "response = route_task(\"summarize this: I love the sun, it's so warm\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "yHlm_A6SPbj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This node is my classifier. It looks at the user's input and decides\n",
        "# what kind of task they’re asking for, so I can route it to the right branch.\n",
        "\n",
        "def router_node(state: RouterState) -> RouterState:\n",
        "    routing_prompt = f\"\"\"\n",
        "You are an AI task classifier.\n",
        "\n",
        "Decide whether the user wants to:\n",
        "- \"summarize\" a passage\n",
        "- or \"translate\" text into French\n",
        "\n",
        "Respond with just one word: summarize or translate.\n",
        "\n",
        "User Input:\n",
        "{state['user_input']}\n",
        "\"\"\"\n",
        "\n",
        "    # Reusing the helper I defined earlier so that the routing logic stays in one place.\n",
        "    task_type = route_task(state[\"user_input\"])\n",
        "\n",
        "    # I store the decision in 'task_type' so the next node can use it for routing.\n",
        "    return {**state, \"task_type\": task_type}\n"
      ],
      "metadata": {
        "id": "0GUmTHWBPbgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This small helper just tells LangGraph which branch to follow next\n",
        "# based on the decision I stored in the state.\n",
        "\n",
        "def router(state: RouterState) -> str:\n",
        "    return state[\"task_type\"]\n"
      ],
      "metadata": {
        "id": "atyHcM_qQLG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This node handles the \"summarize\" path.\n",
        "# It takes the user input and asks the model for a short summary.\n",
        "\n",
        "def summarize_node(state: RouterState) -> RouterState:\n",
        "    prompt = f\"Please summarize the following passage:\\n\\n{state['user_input']}\"\n",
        "    response = llm(prompt)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"task_type\": \"summarize\",\n",
        "        \"output\": response\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2_Vk1fDuQLDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This node handles the \"translate\" path.\n",
        "# It asks the model to translate the input text into French.\n",
        "\n",
        "def translate_node(state: RouterState) -> RouterState:\n",
        "    prompt = f\"Translate the following text to French:\\n\\n{state['user_input']}\"\n",
        "    response = llm(prompt)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"task_type\": \"translate\",\n",
        "        \"output\": response\n",
        "    }\n"
      ],
      "metadata": {
        "id": "PZq-WRE0QLA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I’m creating a new workflow for the routing example.\n",
        "# This graph will take user input, classify the task, and then run the right branch.\n",
        "\n",
        "workflow = StateGraph(RouterState)\n"
      ],
      "metadata": {
        "id": "BrytmfOrQK-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the three main nodes for this routing workflow:\n",
        "# 1) router → decides the task type\n",
        "# 2) summarize → handles summaries\n",
        "# 3) translate → handles French translations\n",
        "\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"summarize\", summarize_node)\n",
        "workflow.add_node(\"translate\", translate_node)\n"
      ],
      "metadata": {
        "id": "k6I2HrjVQK71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The workflow always starts by routing the user input,\n",
        "# so the router node becomes my entry point.\n",
        "\n",
        "workflow.set_entry_point(\"router\")\n"
      ],
      "metadata": {
        "id": "k0IsiwD8QK5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I’m wiring up conditional edges.\n",
        "# Based on the router's decision, the graph will either go to \"summarize\" or \"translate\".\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    router,\n",
        "    {\n",
        "        \"summarize\": \"summarize\",\n",
        "        \"translate\": \"translate\",\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "2fLfl_35QKsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Both summarize and translate are valid end states for this workflow,\n",
        "# so I’m marking them as finish points.\n",
        "\n",
        "workflow.set_finish_point(\"summarize\")\n",
        "workflow.set_finish_point(\"translate\")\n"
      ],
      "metadata": {
        "id": "jTWsC1_CRT8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the routing workflow into an executable LangGraph app.\n",
        "\n",
        "app = workflow.compile()\n"
      ],
      "metadata": {
        "id": "6s5wNWWtRT4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Visualizing the routing graph so I can quickly confirm that\n",
        "# the router branches into the two task-specific nodes as expected.\n",
        "\n",
        "display(Image(app.get_graph().draw_png()))\n"
      ],
      "metadata": {
        "id": "D0YoSTIKRT2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First test: I’m asking the workflow to handle a translation-style request.\n",
        "\n",
        "input_text = {\n",
        "    \"user_input\": \"Can you translate this sentence: I love programming?\"\n",
        "}\n",
        "\n",
        "result = app.invoke(input_text)\n"
      ],
      "metadata": {
        "id": "wZjJ8MwcRTzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking what the workflow produced for the first test.\n",
        "\n",
        "print(result[\"output\"])\n",
        "print(result[\"task_type\"])\n"
      ],
      "metadata": {
        "id": "3Ei6SYo5RTxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second test: now I’m asking for a summary instead of a translation.\n",
        "\n",
        "input_text = {\n",
        "    \"user_input\": (\n",
        "        \"Can you summarize this sentence: I love programming so much, \"\n",
        "        \"it is the best thing ever. All I want to do is programming?\"\n",
        "    )\n",
        "}\n",
        "\n",
        "result = app.invoke(input_text)\n"
      ],
      "metadata": {
        "id": "WYusCs8gRTuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And again, I’m checking the output and the detected task type.\n",
        "\n",
        "print(result[\"output\"])\n",
        "print(result[\"task_type\"])\n"
      ],
      "metadata": {
        "id": "rIofCZnrRTsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I’m defining the state for a simple fan-out translation workflow.\n",
        "# The graph will take some text, translate it into three languages in parallel,\n",
        "# and then combine everything into a single output string.\n",
        "\n",
        "class State(TypedDict):\n",
        "    text: str\n",
        "    french: str\n",
        "    spanish: str\n",
        "    japanese: str\n",
        "    combined_output: str\n"
      ],
      "metadata": {
        "id": "DWKQks-zRTpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This node handles the French translation step.\n",
        "\n",
        "def translate_french(state: State) -> dict:\n",
        "    prompt = f\"Translate the following text to French:\\n\\n{state['text']}\"\n",
        "    response = llm(prompt)\n",
        "    return {\"french\": response.strip()}\n"
      ],
      "metadata": {
        "id": "QooLx-sGRThu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This node handles the Spanish translation step.\n",
        "\n",
        "def translate_spanish(state: State) -> dict:\n",
        "    prompt = f\"Translate the following text to Spanish:\\n\\n{state['text']}\"\n",
        "    response = llm(prompt)\n",
        "    return {\"spanish\": response.strip()}\n"
      ],
      "metadata": {
        "id": "kSTZha5PRTZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This node handles the Japanese translation step.\n",
        "\n",
        "def translate_japanese(state: State) -> dict:\n",
        "    prompt = f\"Translate the following text to Japanese:\\n\\n{state['text']}\"\n",
        "    response = llm(prompt)\n",
        "    return {\"japanese\": response.strip()}\n"
      ],
      "metadata": {
        "id": "b3BqRmkCRTRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once all three translations are done, this node stitches everything together\n",
        "# into one combined, readable output.\n",
        "\n",
        "def aggregator(state: State) -> dict:\n",
        "    combined = f\"Original Text: {state['text']}\\n\\n\"\n",
        "    combined += f\"French: {state['french']}\\n\\n\"\n",
        "    combined += f\"Spanish: {state['spanish']}\\n\\n\"\n",
        "    combined += f\"Japanese: {state['japanese']}\\n\"\n",
        "    return {\"combined_output\": combined}\n"
      ],
      "metadata": {
        "id": "ET8JLiVRRTJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new graph for the parallel translation workflow.\n",
        "\n",
        "graph = StateGraph(State)\n"
      ],
      "metadata": {
        "id": "WsMmxIMcRS-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding all the translation nodes plus the aggregator to the graph.\n",
        "\n",
        "graph.add_node(\"translate_french\", translate_french)\n",
        "graph.add_node(\"translate_spanish\", translate_spanish)\n",
        "graph.add_node(\"translate_japanese\", translate_japanese)\n",
        "graph.add_node(\"aggregator\", aggregator)\n"
      ],
      "metadata": {
        "id": "EmpyRr-ORS6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From the START node, I’m fanning out into three parallel translation branches.\n",
        "\n",
        "graph.add_edge(START, \"translate_french\")\n",
        "graph.add_edge(START, \"translate_spanish\")\n",
        "graph.add_edge(START, \"translate_japanese\")\n"
      ],
      "metadata": {
        "id": "tKkHQX2KSVHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once each translation is done, all three branches feed into the aggregator node.\n",
        "\n",
        "graph.add_edge(\"translate_french\", \"aggregator\")\n",
        "graph.add_edge(\"translate_spanish\", \"aggregator\")\n",
        "graph.add_edge(\"translate_japanese\", \"aggregator\")\n"
      ],
      "metadata": {
        "id": "0l7FEJ8DSVAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The aggregator is the final step before the workflow completes.\n",
        "\n",
        "graph.add_edge(\"aggregator\", END)\n"
      ],
      "metadata": {
        "id": "0zoygSKoSU9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling this graph into a runnable LangGraph app.\n",
        "# Reusing the name 'app' here for simplicity.\n",
        "\n",
        "app = graph.compile()\n"
      ],
      "metadata": {
        "id": "ERPFgfB3SU58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test to see how the full fan-out + aggregation pipeline behaves.\n",
        "\n",
        "input_text = {\n",
        "    \"text\": \"Good morning! I hope you have a wonderful day.\"\n",
        "}\n",
        "\n",
        "result = app.invoke(input_text)\n"
      ],
      "metadata": {
        "id": "5fJVdJheSU2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the combined output with all three translations.\n",
        "\n",
        "result\n"
      ],
      "metadata": {
        "id": "O2J2madHSUzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1BozxDqSUwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}